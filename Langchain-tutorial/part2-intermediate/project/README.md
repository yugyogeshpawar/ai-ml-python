# Part 2 Project: Question-Answering Chatbot with RAG

In this project, we will combine the concepts we've learned in Part 2 to build a Question-Answering chatbot that can answer questions based on the content of a specific document. This project will demonstrate the power of Retrieval-Augmented Generation (RAG).

## Objective

The goal is to create a chain that:
1.  Loads the content of a PDF document.
2.  Splits the document into chunks.
3.  Embeds the chunks and stores them in a vector store.
4.  Creates a retriever from the vector store.
5.  Takes a question from the user.
6.  Retrieves the most relevant document chunks using the retriever.
7.  "Stuffs" the retrieved chunks into a prompt, along with the user's question.
8.  Sends the prompt to an LLM to generate an answer.

## Step-by-Step Implementation

### 1. Choose a PDF Document

Find a PDF document on your computer or download one from the internet. A good choice would be a research paper, a technical manual, or a chapter from a book.

### 2. Create the Main Script

Now, let's create the main Python script for our application.

**`rag_chatbot.py`**

```python
import os
from langchain_openai import OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_openai import OpenAI
from langchain import hub

# Ensure your OPENAI_API_KEY is set as an environment variable

# Create a dummy PDF file for demonstration
with open("your_document.pdf", "w") as f:
    f.write("This is a dummy PDF.")

# 1. Load the PDF
loader = PyPDFLoader("your_document.pdf")  # Replace with your PDF file
documents = loader.load()

# 2. Split the text into chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
texts = text_splitter.split_documents(documents)

# 3. Create embeddings and store them in a VectorStore
embeddings = OpenAIEmbeddings()
db = FAISS.from_documents(texts, embeddings)

# 4. Create a Retriever
retriever = db.as_retriever()

# 5. Create a Chain to answer questions
retrieval_qa_chat_prompt = hub.pull("langchain-ai/retrieval-qa-chat")
combine_docs_chain = create_stuff_documents_chain(
    OpenAI(), retrieval_qa_chat_prompt
)
retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)


# 6. Ask a question!
query = "What are the key findings of this paper?"
result = retrieval_chain.invoke({"input": query})

print("Question:", query)
print("Answer:", result["answer"])
print("\nSource Documents:")
for doc in result["context"]:
    print(doc.metadata["source"])
```

### 3. How to Run the Project

1.  Make sure you have your `OPENAI_API_KEY` environment variable set.
2.  Install the required packages:

    ```bash
    pip install langchain langchain-openai pypdf tiktoken faiss-cpu
    ```
3.  Replace `"your_document.pdf"` with the actual path to your PDF file.
4.  Navigate to the `project` directory in your terminal.
5.  Run the script:

    ```bash
    python rag_chatbot.py
    ```

6.  The script will print the answer to your question, along with the source documents used to generate the answer.

### Example Usage

```
Question: What are the key findings of this paper?
Answer: The key findings of this paper are... (generated by the LLM based on the retrieved documents)

Source Documents:
your_document.pdf (page 1)
your_document.pdf (page 5)
your_document.pdf (page 8)
```

This project demonstrates how to combine Document Loaders, Text Splitters, Embeddings, Vector Stores, Retrievers, and Chains to build a powerful Question-Answering chatbot.
