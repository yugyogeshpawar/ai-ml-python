# Part 2 Project: Question-Answering Chatbot with RAG

In this project, we will combine the concepts we've learned in Part 2 to build a Question-Answering chatbot that can answer questions based on the content of a specific document. This project will demonstrate the power of Retrieval-Augmented Generation (RAG).

## Objective

The goal is to create a chain that:
1.  Loads the content of a PDF document.
2.  Splits the document into chunks.
3.  Embeds the chunks and stores them in a vector store.
4.  Creates a retriever from the vector store.
5.  Takes a question from the user.
6.  Retrieves the most relevant document chunks using the retriever.
7.  "Stuffs" the retrieved chunks into a prompt, along with the user's question.
8.  Sends the prompt to an LLM to generate an answer.

## Step-by-Step Implementation

### 1. Choose a PDF Document

Find a PDF document on your computer or download one from the internet. A good choice would be a research paper, a technical manual, or a chapter from a book.

### 2. Create the Main Script

Now, let's create the main Python script for our application.

**`rag_chatbot.py`**

```python
import os
from langchain_openai import OpenAIEmbeddings
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_openai import OpenAI

# 1. Load the PDF
loader = PyPDFLoader("your_document.pdf")  # Replace with your PDF file
documents = loader.load()

# 2. Split the text into chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
texts = text_splitter.split_documents(documents)

# 3. Create embeddings and store them in a VectorStore
embeddings = OpenAIEmbeddings()
db = FAISS.from_documents(texts, embeddings)

# 4. Create a Retriever
retriever = db.as_retriever()

# 5. Create a Chain to answer questions
qa = RetrievalQA.from_chain_type(
    llm=OpenAI(),
    chain_type="stuff",  # "stuff" method simply concatenates all retrieved docs
    retriever=retriever,
    return_source_documents=True # Returns the docs used to answer
)

# 6. Ask a question!
query = "What are the key findings of this paper?"
result = qa({"query": query})

print("Question:", query)
print("Answer:", result["result"])
print("\nSource Documents:")
for doc in result["source_documents"]:
    print(doc.metadata["source"])
```

### 3. How to Run the Project

1.  Make sure you have your `OPENAI_API_KEY` environment variable set.
2.  Install the required packages:

    ```bash
    pip install langchain langchain-openai pypdf tiktoken faiss-cpu
    ```
3.  Replace `"your_document.pdf"` with the actual path to your PDF file.
4.  Navigate to the `project` directory in your terminal.
5.  Run the script:

    ```bash
    python rag_chatbot.py
    ```

6.  The script will print the answer to your question, along with the source documents used to generate the answer.

### Example Usage

```
Question: What are the key findings of this paper?
Answer: The key findings of this paper are... (generated by the LLM based on the retrieved documents)

Source Documents:
your_document.pdf (page 1)
your_document.pdf (page 5)
your_document.pdf (page 8)
```

This project demonstrates how to combine Document Loaders, Text Splitters, Embeddings, Vector Stores, Retrievers, and Chains to build a powerful Question-Answering chatbot.
